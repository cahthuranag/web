<!DOCTYPE html>
<!-- saved from url=(0038)https://people.csail.mit.edu/pulkitag/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script type="text/javascript" async="" src="./Pulkit Agrawal_files/analytics.js.download"></script><script async="" src="./Pulkit Agrawal_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-151308353-1');
</script>

<script src="./Pulkit Agrawal_files/jquery.min.js.download">

<! script src="resources/authors.js"></script>

<script>
var Author = {
    //default config
    config: {
        el: '#author',
        name: 'Author Name',
        url: 'empty.com'
    },

    init: function (config) {
      var cfg = this.config = $.extend({}, this.config, config);
      $(cfg.el).html('<a href=' + cfg.url + '>');
      $(cfg.el).innerHTML = cfg.name;
    }
};

$(function () {
    Object.create(Author).init({
        el: '#taochen',
        name: "Tao Chen",
        url: "https://taochenshh.github.io/"
    });

    Object.create(Author).init({
        el: '#jacobhuh',
        name: 'Jacob Huh',
        url: "http://minyounghuh.com/"
    });
});
</script>

  <title>Pulkit Agrawal</title>

  <meta name="author" content="Pulkit Agrawal">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="./Pulkit Agrawal_files/stylesheet.css">
  <link rel="icon" type="image/jpg" href="./Pulkit Agrawal_files/pulkit.jpg">
<style>._3emE9--dark-theme .-S-tR--ff-downloader{background:rgba(30,30,30,.93);border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#fff}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{background:#3d4b52}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#131415}._3emE9--dark-theme .-S-tR--ff-downloader ._10vpG--footer{background:rgba(30,30,30,.93)}._2mDEx--white-theme .-S-tR--ff-downloader{background:#fff;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#314c75}._2mDEx--white-theme .-S-tR--ff-downloader ._6_Mtt--header{font-weight:700}._2mDEx--white-theme .-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{border:0;color:rgba(0,0,0,.88)}._2mDEx--white-theme .-S-tR--ff-downloader ._10vpG--footer{background:#fff}.-S-tR--ff-downloader{display:block;overflow:hidden;position:fixed;bottom:20px;right:7.1%;width:330px;height:180px;background:rgba(30,30,30,.93);border-radius:2px;color:#fff;z-index:99999999;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);transition:.5s}.-S-tR--ff-downloader._3M7UQ--minimize{height:62px}.-S-tR--ff-downloader._3M7UQ--minimize .nxuu4--file-info,.-S-tR--ff-downloader._3M7UQ--minimize ._6_Mtt--header{display:none}.-S-tR--ff-downloader ._6_Mtt--header{padding:10px;font-size:17px;font-family:sans-serif}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{float:right;background:#f1ecec;height:20px;width:20px;text-align:center;padding:2px;margin-top:-10px;cursor:pointer}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#e2dede}.-S-tR--ff-downloader ._13XQ2--error{color:red;padding:10px;font-size:12px;line-height:19px}.-S-tR--ff-downloader ._2dFLA--container{position:relative;height:100%}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info{padding:6px 15px 0;font-family:sans-serif}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info div{margin-bottom:5px;width:100%;overflow:hidden}.-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{margin-top:21px;font-size:11px}.-S-tR--ff-downloader ._10vpG--footer{width:100%;bottom:0;position:absolute;font-weight:700}.-S-tR--ff-downloader ._10vpG--footer ._2V73d--loader{-webkit-animation:n0BD1--rotation 3.5s linear forwards;animation:n0BD1--rotation 3.5s linear forwards;position:absolute;top:-120px;left:calc(50% - 35px);border-radius:50%;border:5px solid #fff;border-top-color:#a29bfe;height:70px;width:70px;display:flex;justify-content:center;align-items:center}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar{width:100%;height:18px;background:#dfe6e9;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar ._1FVu9--progress-bar{height:100%;background:#8bc34a;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status{margin-top:10px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1XilH--state{float:left;font-size:.9em;letter-spacing:1pt;text-transform:uppercase;width:100px;height:20px;position:relative}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1jiaj--percentage{float:right}._3vFA8--container{height:0;width:100%;position:relative;background:radial-gradient(44.6% 929.87% at 22.02% 63.39%,#ae6fff 0,#8d35fd 100%);cursor:pointer;overflow:hidden;transition:.8s}._3vFA8--container .AeJ0I--row{padding:11px;margin:0 auto;width:-webkit-max-content;width:-moz-max-content;width:max-content}._3vFA8--container .AeJ0I--row ._1yCun--logo{position:relative;width:51px;height:39px;float:left;margin-right:104px;background:url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNTAiIGhlaWdodD0iNDEiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0xNy40MDQgOC43TDEuNDU0IDM2LjMyNWMtLjI1OC40NDctLjM4Ny42Ny0uNDUxLjgzOWEyLjIzNiAyLjIzNiAwIDAwMS43MzMgMy4wMDFjLjE3OC4wMy40MzYuMDMuOTUyLjAzLjEwNiAwIC4xNTggMCAuMjEtLjAwM2EyLjIzNyAyLjIzNyAwIDAwMS4yNTItLjQ1Mmw2Ljg1OS00LjcyMWMuNDczLS4zLjkwOC0uNjU0IDEuMjk2LTEuMDUyLjM1NC4wOC43MjcuMTg1IDEuMTQ1LjMwMSAxLjk1LjU0NCA0Ljg4MyAxLjM2IDExLjUxNiAxLjM2IDcuNjcyIDAgMTAuNjEtLjg3NCAxMi4xNTQtMS40NzEuMzguMzQzLjc5Ni42NDYgMS4yNDQuOTA0bDcuNDg4IDQuODE0Yy4zMjQuMjA4LjcwMi4zMiAxLjA4OC4zMiAxLjU1IDAgMi41MTgtMS42NzggMS43NDMtMy4wMkwzMy4yNDIgOC43Yy0yLjU3LTQuNDUyLTMuODU2LTYuNjc5LTUuNTI1LTcuNDQzYTUuNzUxIDUuNzUxIDAgMDAtNC43ODggMGMtMS42NjkuNzY0LTIuOTU0IDIuOTktNS41MjUgNy40NDN6bTguNTYyIDIyLjUyNGMtNC4yNDYgMC03LjU0OC0uNTk4LTkuOTI1LTEuNTM0bC4wMDEtLjAwMmMtLjU2MS0uMjQtMS40MjItLjc4My0xLjIyOC0xLjI5NS4xOTctLjUyIDEuNDQ0LS41MDcgMi4xOC0uMzRsNS4wNzYtOC43MiAzLjU1LTIuNTQ0LTIuMjUuMzEuMDE0LS4wMjMtMi40NTMuMjdjLS40NC4wNS0uNjU4LjA3My0uNzU0LjAwNGEuMzA0LjMwNCAwIDAxLS4xMjMtLjI3MmMuMDEtLjExNy4xNzItLjI2Ni40OTYtLjU2NCAxLjMwNS0xLjE5OCAzLjM3Mi0zLjA5IDMuNzQ3LTMuMzk4YTEuNDM4IDEuNDM4IDAgMDEyLjIwMy40NzhsNy4zMTcgMTMuNDc4Yy45MjItLjI0NCAyLjMwOC0uMzU2IDIuNDU1LjQ4OC4xMy43NDMtLjYwNyAxLjI1OS0yLjM2NyAyLjAyNC0xLjcyMy44LTQuNjY3IDEuNjQtNy45MzkgMS42NHoiIGZpbGw9IiNmZmYiLz48L3N2Zz4=)}._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._1ErQY--star-1,._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._2JsUk--star-2,._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._181pR--star-3{position:absolute}._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._1ErQY--star-1{background:url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTIwIiBoZWlnaHQ9IjU2IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnIGZpbHRlcj0idXJsKCNmaWx0ZXIwX2QpIj48cGF0aCBkPSJNNTUuNjU1IDM3LjAyNWMuNDc0LTEuNjMgMi41ODYtMi4wNTUgMy42NTMtLjczNWwzLjU4NSA0LjQzNWMuNDE0LjUxMiAxLjA0My44IDEuNy43OGw1LjctLjE3OGMxLjY5OC0uMDUzIDIuNzU1IDEuODI0IDEuODI5IDMuMjQ4bC0zLjExIDQuNzhhMi4xMDIgMi4xMDIgMCAwMC0uMjE3IDEuODU4bDEuOTMxIDUuMzY2Yy41NzUgMS41OTgtLjg4NCAzLjE4My0yLjUyNCAyLjc0MmwtNS41MDctMS40ODFhMi4xMDIgMi4xMDIgMCAwMC0xLjgzNC4zNjlsLTQuNTA3IDMuNDk0Yy0xLjM0MiAxLjA0LTMuMy4xNDMtMy4zODctMS41NTNsLS4yOTQtNS42OTVhMi4xMDIgMi4xMDIgMCAwMC0uOTE3LTEuNjNsLTQuNzE3LTMuMjA3Yy0xLjQwNC0uOTU1LTEuMTU1LTMuMDk0LjQzLTMuNzAybDUuMzI3LTIuMDM4YTIuMTAyIDIuMTAyIDAgMDAxLjI2Ny0xLjM3N2wxLjU5Mi01LjQ3NnoiIGZpbGw9IiNGRkQ2MDAiLz48L2c+PGRlZnM+PGZpbHRlciBpZD0iZmlsdGVyMF9kIiB4PSIuMTI3IiB5PSItMTEuMDkiIHdpZHRoPSIxMTkuMzI3IiBoZWlnaHQ9IjExOC40NjIiIGZpbHRlclVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgY29sb3ItaW50ZXJwb2xhdGlvbi1maWx0ZXJzPSJzUkdCIj48ZmVGbG9vZCBmbG9vZC1vcGFjaXR5PSIwIiByZXN1bHQ9IkJhY2tncm91bmRJbWFnZUZpeCIvPjxmZUNvbG9yTWF0cml4IGluPSJTb3VyY2VBbHBoYSIgdmFsdWVzPSIwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAxMjcgMCIvPjxmZU9mZnNldC8+PGZlR2F1c3NpYW5CbHVyIHN0ZERldmlhdGlvbj0iMjIiLz48ZmVDb2xvck1hdHJpeCB2YWx1ZXM9IjAgMCAwIDAgMSAwIDAgMCAwIDEgMCAwIDAgMCAxIDAgMCAwIDAuNSAwIi8+PGZlQmxlbmQgaW4yPSJCYWNrZ3JvdW5kSW1hZ2VGaXgiIHJlc3VsdD0iZWZmZWN0MV9kcm9wU2hhZG93Ii8+PGZlQmxlbmQgaW49IlNvdXJjZUdyYXBoaWMiIGluMj0iZWZmZWN0MV9kcm9wU2hhZG93IiByZXN1bHQ9InNoYXBlIi8+PC9maWx0ZXI+PC9kZWZzPjwvc3ZnPg==");width:120px;height:56px;top:-12px;left:-103px;background-size:120px 56px}._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._2JsUk--star-2{background:url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTE1IiBoZWlnaHQ9IjU2IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnIGZpbHRlcj0idXJsKCNmaWx0ZXIwX2QpIj48cGF0aCBkPSJNNTguMzg4LTUuNjljLjg4NS0xLjAyNSAyLjU1Ny0uNjMzIDIuODkzLjY3OUw2Mi40MS0uNjA2Yy4xMy41MDguNDkuOTI2Ljk3NCAxLjEzbDQuMTkzIDEuNzYxYzEuMjQ4LjUyNCAxLjM5MiAyLjIzNi4yNDggMi45NjFsLTMuODQgMi40MzVjLS40NDQuMjgtLjczLjc1My0uNzc0IDEuMjc2bC0uMzggNC41MzFjLS4xMTMgMS4zNS0xLjY5NiAyLjAxNS0yLjc0IDEuMTUxbC0zLjUwMS0yLjlhMS42NzYgMS42NzYgMCAwMC0xLjQ1My0uMzQxbC00LjQyNyAxLjA0Yy0xLjMxOC4zMDktMi40NC0uOTkyLTEuOTQxLTIuMjVsMS42NzYtNC4yMjhhMS42NzYgMS42NzYgMCAwMC0uMTI0LTEuNDg3TDQ3Ljk2NC41ODRjLS43MDItMS4xNTguMTg5LTIuNjI3IDEuNTQtMi41NDFsNC41MzguMjg4YTEuNjc2IDEuNjc2IDAgMDAxLjM3Ni0uNTc4bDIuOTctMy40NDN6IiBmaWxsPSIjRkZENjAwIi8+PC9nPjxkZWZzPjxmaWx0ZXIgaWQ9ImZpbHRlcjBfZCIgeD0iLjk2MiIgeT0iLTUyLjY2OCIgd2lkdGg9IjExMy40NzciIGhlaWdodD0iMTEyLjgyMSIgZmlsdGVyVW5pdHM9InVzZXJTcGFjZU9uVXNlIiBjb2xvci1pbnRlcnBvbGF0aW9uLWZpbHRlcnM9InNSR0IiPjxmZUZsb29kIGZsb29kLW9wYWNpdHk9IjAiIHJlc3VsdD0iQmFja2dyb3VuZEltYWdlRml4Ii8+PGZlQ29sb3JNYXRyaXggaW49IlNvdXJjZUFscGhhIiB2YWx1ZXM9IjAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDEyNyAwIi8+PGZlT2Zmc2V0Lz48ZmVHYXVzc2lhbkJsdXIgc3RkRGV2aWF0aW9uPSIyMiIvPjxmZUNvbG9yTWF0cml4IHZhbHVlcz0iMCAwIDAgMCAxIDAgMCAwIDAgMSAwIDAgMCAwIDEgMCAwIDAgMC41IDAiLz48ZmVCbGVuZCBpbjI9IkJhY2tncm91bmRJbWFnZUZpeCIgcmVzdWx0PSJlZmZlY3QxX2Ryb3BTaGFkb3ciLz48ZmVCbGVuZCBpbj0iU291cmNlR3JhcGhpYyIgaW4yPSJlZmZlY3QxX2Ryb3BTaGFkb3ciIHJlc3VsdD0ic2hhcGUiLz48L2ZpbHRlcj48L2RlZnM+PC9zdmc+");width:114px;height:53px;left:9px;top:-12.63px;background-size:120px 62px}._3vFA8--container .AeJ0I--row ._1yCun--logo ._2GogZ--stars ._181pR--star-3{background:url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAzIiBoZWlnaHQ9IjU2IiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnIGZpbHRlcj0idXJsKCNmaWx0ZXIwX2QpIj48cGF0aCBkPSJNNTEuODAyIDM2LjA5OGMxLjA5OS0xLjQ5MSAzLjQyLTEuMTMyIDQuMDE2LjYyMmwyLjAwMSA1Ljg4N2MuMjMxLjY4Ljc2NyAxLjIxMSAxLjQ0OCAxLjQzN2w1LjkgMS45NjFjMS43NTguNTg0IDIuMTMzIDIuOTAzLjY1IDQuMDExbC00Ljk4IDMuNzIzYTIuMjkyIDIuMjkyIDAgMDAtLjkyIDEuODIxbC0uMDQyIDYuMjE4Yy0uMDEyIDEuODUyLTIuMTAxIDIuOTI2LTMuNjE0IDEuODU4bC01LjA4LTMuNTg3YTIuMjkyIDIuMjkyIDAgMDAtMi4wMTYtLjMxMmwtNS45MjYgMS44ODJjLTEuNzY1LjU2LTMuNDMyLTEuMDk1LTIuODgzLTIuODY0bDEuODQtNS45MzlhMi4yOTIgMi4yOTIgMCAwMC0uMzI1LTIuMDE0bC0zLjYyMi01LjA1NWMtMS4wNzgtMS41MDUtLjAxOS0zLjYwMSAxLjgzMy0zLjYyNmw2LjIxOC0uMDg1YTIuMjkyIDIuMjkyIDAgMDAxLjgxNC0uOTMybDMuNjg4LTUuMDA2eiIgZmlsbD0iI0ZGRDYwMCIvPjwvZz48ZGVmcz48ZmlsdGVyIGlkPSJmaWx0ZXIwX2QiIHg9Ii43ODgiIHk9Ii0xLjUzNCIgd2lkdGg9IjEwMi4xMzUiIGhlaWdodD0iMTAxLjEyOSIgZmlsdGVyVW5pdHM9InVzZXJTcGFjZU9uVXNlIiBjb2xvci1pbnRlcnBvbGF0aW9uLWZpbHRlcnM9InNSR0IiPjxmZUZsb29kIGZsb29kLW9wYWNpdHk9IjAiIHJlc3VsdD0iQmFja2dyb3VuZEltYWdlRml4Ii8+PGZlQ29sb3JNYXRyaXggaW49IlNvdXJjZUFscGhhIiB2YWx1ZXM9IjAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDAgMCAwIDEyNyAwIi8+PGZlT2Zmc2V0Lz48ZmVHYXVzc2lhbkJsdXIgc3RkRGV2aWF0aW9uPSIxNy4yNDYiLz48ZmVDb2xvck1hdHJpeCB2YWx1ZXM9IjAgMCAwIDAgMSAwIDAgMCAwIDEgMCAwIDAgMCAxIDAgMCAwIDAuNSAwIi8+PGZlQmxlbmQgaW4yPSJCYWNrZ3JvdW5kSW1hZ2VGaXgiIHJlc3VsdD0iZWZmZWN0MV9kcm9wU2hhZG93Ii8+PGZlQmxlbmQgaW49IlNvdXJjZUdyYXBoaWMiIGluMj0iZWZmZWN0MV9kcm9wU2hhZG93IiByZXN1bHQ9InNoYXBlIi8+PC9maWx0ZXI+PC9kZWZzPjwvc3ZnPg==");width:114px;height:75px;left:34px;top:-12.63px;background-size:120px 57px}._3vFA8--container .AeJ0I--row ._2CXkc--title{float:left;width:-webkit-max-content;width:-moz-max-content;width:max-content;height:23px;font-family:Montserrat,sans-serif;font-style:normal;font-weight:400;font-size:18px;line-height:37px;color:#fff;text-shadow:0 0 16px #5600c5;margin-right:20px}._3vFA8--container .AeJ0I--row ._1CujY--btn{float:left;flex-direction:row;padding:6px 22px;width:111px;margin-top:-2px;height:38px;background:#ffd600;box-shadow:0 0 28px hsla(0,0%,100%,.5);border-radius:5px}._3vFA8--container .AeJ0I--row ._1CujY--btn ._2Wo2K--btnText{width:66px;height:14px;font-family:Montserrat,serif;font-style:normal;font-weight:600;font-size:11px;line-height:14px;text-align:center;letter-spacing:-.01em;color:#9645fd;align-self:center;margin:6px 0}</style></head>

<body data-gr-c-s-loaded="true">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pulkit Agrawal</name>
              </p>
              <p>I am an Assistant Professor in the department of Electrical
                Engineering and Computer Science (EECS) at <a href="https://people.csail.mit.edu/pulkitag/www.mit.edu">MIT</a>.
                My lab is part of the Computer Science and Artificial Intelligence Lab
                (<a href="https://www.csail.mit.edu/">CSAIL</a>) at MIT and affiliated with
                the Laboratory for Information and Decision Systems
                (<a href="https://lids.mit.edu/">LIDS</a>).
              </p>
              <p>
                I completed my Ph.D. at UC Berkeley; undergraduate studies from IIT Kanpur;
                co-founded <a href="https://www.safely-you.com/">SafelyYou Inc.</a>
                that builds fall prevention technology and the
                <a href="https://aifoundry.ai/">AI Foundry</a>, an incubator for AI startups.
              </p>
              <p>
              </p>
              <p style="text-align:center">
                <a href="mailto:pulkitag@mit.edu">Email</a> &nbsp;/&nbsp;
                <a href="https://people.csail.mit.edu/pulkitag/data/pulkit_CV_current.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://people.csail.mit.edu/pulkitag/data/pulkit-bio.txt">Biography</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=UpZmJI0AAAAJ&amp;hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/pulkit-agrawal-967a4218/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="./Pulkit Agrawal_files/pulkit.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="./Pulkit Agrawal_files/pulkit.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
               The overarching research interest is to build machines that can automatically and continuously
               learn about their environment. The hope is that the end result of such learning will be similar
               to development of what humans call <em>common sense</em>. I refer to this line of work as
               <b>"computational sensorimotor learning"</b>
               and it encompasses <b>computer vision</b>, <b>robotics</b>, <b>reinforcement learning</b>,
               and other learning based approaches to control.
               Some of my past work has also touched upon principles of <b>cognitive science,
               neuroscience</b> to draw upon inspiration from these discplines.
               My key papers are <span class="highlight">highlighted</span>.
              </p>

              <p style="text-align:center">
                <a href="http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-133.pdf">
                  Ph.D. Thesis (Computational Sensorimotor Learning)</a> &nbsp;/&nbsp;
                <a href="https://youtu.be/opsQndkSw5k" savefrom_lm_index="0" savefrom_lm="1">Thesis Talk</a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fyoutu.be%2FopsQndkSw5k&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Group</heading>
              <p>
              The lab is an unsual collection of folks working on something that is unconceivable/unthinkable, but not
              impossible in our lifetime: General Artificial Intelligence. Life is short, do what you must do :-)
              I like to call my group: <b>Improbable AI Lab</b>.
              </p>
              <p>
              </p><p>
                <b>PostDocs</b> <br>
                <a href="https://scholar.google.com/citations?user=7N-ethYAAAAJ&amp;hl=en"> Brian Cheung </a>

              </p>
              <b> Ph.D. Students </b> <br>
              <a href="https://taochenshh.github.io/">Tao Chen</a> <br>
              <a href="https://anthonysimeonov.github.io/"> Anthony Simeonov </a> (co-advised with Alberto Rodriguez) <br>
              <a href="http://minyounghuh.com/"> Jacob Huh </a> (co-advised with Phillip Isola) <br>
              <a href="https://www.csail.mit.edu/person/yanwei-wang"> Felix Wang </a> <br>
              <a href="https://anuragajay.github.io/"> Anurag Ajay </a> <br>
            <p></p>
            <p>
            <b> Research Assistants </b> <br>
              <a href="https://richardrl.github.io/"> Richard Li </a> <br>
              Blake Tickell <br>
            </p>
            <p>
            <b>Undergraduate Researchers </b> <br>
              Alon Z. Kosowsky-Sachs <br>
              Eric Chen <br>
              Julia Gonik <br>
              Joshua Gruenstein <br>
            </p>
            <p>
            <b>Openings</b> <br>
              We have openings for <b>Ph.D. Students, PostDocs, and MIT UROPs/SuperUROPs</b>.
              If you would like to apply for the Ph.D. program, please apply directly
              to MIT EECS admissions. For all other positions, send me an e-mail with your resume.
            </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <a href="https://richardrl.github.io/relational-rl/">
              <img src="./Pulkit Agrawal_files/block-stacking-2019.gif" alt="sym" width="100%" style="border-radius:5px">
          </a></td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://richardrl.github.io/relational-rl/">
              <papertitle>Towards Practical Multi-object Manipulation using
Relational Reinforcement Learning</papertitle>
            </a>
            <br>
            <a href="https://richardrl.github.io/">Richard Li</a>,
            <a href="https://ajabri.github.io/">Allan Jabri</a>,
            <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
            <strong>Pulkit Agrawal</strong>
            <br>
            (in submission)
            <br>
            <br>
            <a href="https://arxiv.org/pdf/1912.11032.pdf">paper</a> /
            <a href="https://richardrl.github.io/relational-rl/">website</a> /
            <a href="https://github.com/richardrl/rlkit-relational">code</a> /
            <a href="https://people.csail.mit.edu/pulkitag/data/li2019towards.bib">bibtex</a>
            <p></p>
            <p> Combining graph neural networks with curriculum learning for solve
            long horizon multi-object manipulation tasks.</p>
          </td>
        </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.05522">
                <img src="./Pulkit Agrawal_files/superposition_nutshell.png" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.05522">
                <papertitle>Superposition of Many Models into One</papertitle>
              </a>
              <br>
              <a href="https://redwood.berkeley.edu/people/brian-cheung/">Brian Cheung</a>,
              <a href="https://redwood.berkeley.edu/people/alex-terekhov/">Alex Terekhov</a>,
              <a href="https://redwood.berkeley.edu/people/yubei-chen/">Yubei Chen</a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="http://www.rctn.org/bruno/">Bruno Olshausen</a>,
              <br>
              <em>NeurIPS </em>, 2019
              <br>
              <br>
              <a href="https://arxiv.org/abs/1902.05522">arxiv</a> /
              <a href="https://www.youtube.com/watch?v=1WopZJ4WrX0" savefrom_lm_index="0" savefrom_lm="1">video tutorial</a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D1WopZJ4WrX0&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span> /
              <a href="https://github.com/briancheung/superposition">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/cheung2019superposition.bib">bibtex</a>
              <p></p>
              <p> A method for storing multiple neural network models for different
                tasks into a single neural network.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://www.ncbi.nlm.nih.gov/pubmed/31318502">
                <img src="./Pulkit Agrawal_files/emt_2019.png" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ajmc.s3.amazonaws.com/_media/_pdf/AJMC_07_2019_Xiong%20final.pdf">
                <papertitle>Real-time Video Detection of Falls in Dementia Care
                  Facility and Reduced Emergency Care</papertitle>
              </a>
              <br>
              Glen L Xiong, Eleonore Bayen, Shirley Nickels, Raghav Subramaniam,
              <strong> Pulkit Agrawal</strong>, Julien Jacquemot, Alexandre M Bayen,
              Bruce Miller, George Netscher
              <br>
              <em>American Journal of Managed Care </em>, 2019
              <br>
              <br>
              <a href="https://ajmc.s3.amazonaws.com/_media/_pdf/AJMC_07_2019_Xiong%20final.pdf">paper</a> /
              <a href="https://www.safely-you.com/">SafelyYou</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/xiong2019real.bib">bibtex</a>
              <p></p>
              <p> Computer Vision based Fall Detection system reduces number of
              falls and emergency room visits in people with Dementia.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=BkisuzWRW">
                <img src="./Pulkit Agrawal_files/iclr18_1.gif" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=BkisuzWRW">
                <papertitle>Zero Shot Visual Imitation</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~pathak/">Deepak Pathak*</a>,
              <a href="https://people.eecs.berkeley.edu/~parsa.m/">Parsa Mahmoudieh*</a>,
              Michael Luo,
              <strong>Pulkit Agrawal*</strong>, <br>
              <a href="https://people.eecs.berkeley.edu/~shelhamer/">Evan Shelhamer</a>,
              <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              (* equal contribution)
              <br>
              <em>ICLR</em>, 2018 &nbsp; <font color="red"><strong>(Oral)</strong></font>
              <br>
              <br>
              <a href="https://openreview.net/forum?id=BkisuzWRW">paper</a> /
              <a href="https://pathak22.github.io/zeroshot-imitation/">website</a> /
              <a href="https://github.com/pathak22/zeroshot-imitation">code</a> /
              <a href="https://www.dropbox.com/s/36efg1t3qn6i495/2018_04_ZeroShotImitation.pptx">slides</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/pathak2018zero.bib">bibtex</a>
              <p></p>
              <p> Self-supervised learning of skills helps an agent imitate
              the task presented as a sequence of images. Forward consistency loss
              overcomes key challenges of inverse and forward models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://rach0012.github.io/humanRL_website/">
                <img src="./Pulkit Agrawal_files/icml18.png" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://rach0012.github.io/humanRL_website/">
                <papertitle>Investigating Human Priors for Playing Video Games</papertitle>
              </a>
              <br>
              <a href="http://cocosci.princeton.edu/rachit/">Rachit Dubey</a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="https://people.eecs.berkeley.edu/~pathak/">Deepak Pathak</a>,
              <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
              <a href="http://cocosci.princeton.edu/tom/">Tom Griffiths</a>
              <br>
              <em>ICML</em>, 2018
              <br>
              <br>
              <a href="https://arxiv.org/pdf/1802.10217.pdf">paper</a> /
              <a href="https://rach0012.github.io/humanRL_website/">website</a> /
              <a href="https://youtu.be/Ol0-c9OE3VQ" savefrom_lm_index="0" savefrom_lm="1">youtube cover</a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fyoutu.be%2FOl0-c9OE3VQ&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span> /
              <a href="https://rach0012.github.io/humanRL_website/#media">media</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/dubey2018investigating.bib">bibtex</a>
              <p></p>
              <p> An empirical study of various kinds of prior information used
                by humans to solve video games. Such priors make them significantly
              more sample efficient as compared to Deep Reinforcement Learning algorithms.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://pathak22.github.io/seg-by-interaction/">
                <img src="./Pulkit Agrawal_files/instance_segmentation.png" alt="sym" width="100%" style="border-radius:5px">
            </a></td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pathak22.github.io/seg-by-interaction/">
                <papertitle>Learning Instance Segmentation by Interaction</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~pathak/">Deepak Pathak*</a>,
              Yide Shentu*,
              <a href="http://www.cs.utexas.edu/~dchen/">Dian Chen*</a>,
              <strong>Pulkit Agrawal*</strong>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              &nbsp; (*equal contribution)
              <br>
              <em>CVPR Workshop</em>, 2018
              <br>
              <br>
              <a href="https://arxiv.org/pdf/1806.08354.pdf">paper</a> /
              <a href="https://pathak22.github.io/seg-by-interaction/">website</a>
              <a href="https://people.csail.mit.edu/pulkitag/data/pathak2018learning.bib">bibtex</a>
              <p></p>
              <p> A self-supervised method for learning to segment objects by
                interacting with them. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.118.034338">
                <img src="./Pulkit Agrawal_files/circulation18_2.png" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.118.034338">
                <papertitle>Fully Automated Echocardiogram Interpretation in Clinical Practice:
                  Feasibility and Diagnostic Accuracy </papertitle>
              </a>
              <br>
              Jeffrey Zhang, Sravani Gajjala, <strong>Pulkit Agrawal</strong>, Geoffrey H Tison, Laura A Hallock,
              Lauren Beussink-Nelson, Mats H Lassen, Eugene Fan, Mandar A Aras, ChaRandle Jordan,
              Kirsten E Fleischmann, Michelle Melisko, Atif Qasim, Sanjiv J Shah,
              Ruzena Bajcsy, Rahul C Deo
              <br>
              <em>Circulation</em>, 2018
              <br>
              <br>
              <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.118.034338">paper</a> /
              <a href="https://arxiv.org/abs/1706.07342">arxiv</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/zhang2018fully.bib">bibtex</a>
              <p></p>
              <p> Computer vision method for building fully automated and scalable analysis
                pipeline for echocardiogram interpretation.</p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="http://pathak22.github.io/noreward-rl/">
                <img src="./Pulkit Agrawal_files/icml17_1.gif" alt="sym" width="100%" style="border-radius:5px">
            </a></td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://people.csail.mit.edu/pulkitag/TODO">
                <papertitle>Curiosity Driven Exploration by Self-Supervised Prediction</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~pathak/">Deepak Pathak</a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
              <em>ICML</em>, 2017
              <br>
              <br>
              <a href="https://arxiv.org/abs/1705.05363">arxiv</a> /
              <a href="https://www.youtube.com/watch?v=J3FHOyhUn3A" savefrom_lm_index="0" savefrom_lm="1">video</a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DJ3FHOyhUn3A&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span> /
              <a href="https://vimeo.com/237270588">talk</a> /
              <a href="https://github.com/pathak22/noreward-rl">code</a> /
              <a href="https://pathak22.github.io/noreward-rl/"> project website </a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/pathak2017curiosity.bib">bibtex</a>
              <p></p>
              <p> Intrinsic curiosity of agents enables them to learn useful and
                generalizable skills without any rewards from the environment. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Felsen_What_Will_Happen_ICCV_2017_paper.html">
                  <img src="./Pulkit Agrawal_files/waterpolo.png" alt="sym" width="100%" style="border-radius:5px">
              </a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Felsen_What_Will_Happen_ICCV_2017_paper.html">
              <papertitle>What Will Happen Next?: Forecasting Player Moves in Sports Videos</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/panna-felsen-030a3964">Panna Felsen</a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>ICCV</em>, 2017
              <br>
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Felsen_What_Will_Happen_ICCV_2017_paper.pdf">paper </a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/felsen2017iccv.bib">bibtex</a>
              <p></p>
              <p> Feature learning by making use of an agent's knowledge of its motion.</p>
            </td>
          </tr>


          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/rope-manipulation.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ropemanipulation.github.io/">
              <papertitle>Combining Self-Supervised Learning and Imitation for Vision-based Rope Manipulation</papertitle>
              </a>
              <br>
              <a href="http://ashvin.me/">Ashvin Nair*</a>,
              <a href="http://www.cs.utexas.edu/~dchen/">Dian Chen*</a>,
              <strong>Pulkit Agrawal*</strong>,
              <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>(*equal contribution)
              <br>
              <em>ICRA</em>, 2017
              <br>
              <br>
              <a href="https://arxiv.org/abs/1703.02018">arxiv</a> /
              <a href="https://ropemanipulation.github.io/"> website</a> /
              <a href="https://youtu.be/ofNQh5ELrOw" savefrom_lm_index="0" savefrom_lm="1"> video </a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fyoutu.be%2FofNQh5ELrOw&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span> /
              <a href="https://people.csail.mit.edu/pulkitag/data/nair2017combining.bib">bibtex</a>
              <p></p>
              <p> Self-supervised learning of low-level skills enables a robot to
                follow a high-level plan specified by a single video demonstration.
                The code for the paper <em> Zero Shot Visual Imitation </em>
                subsumes this project's code release. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/physics_exp.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1611.01843">
              <papertitle>Learning to Perform Physics Experiments via Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <a href="http://mdenil.com/">Misha Denil</a>,
              <strong>Pulkit Agrawal*</strong>,
              <a href="https://tejasdkulkarni.github.io/">Tejas D Kulkarni</a>,
              <a href="https://scholar.google.com/citations?user=gVFnjOcAAAAJ&amp;hl=en">Tom Erez</a>,
              <a href="https://scholar.google.com/citations?user=nQ7Ij30AAAAJ&amp;hl=en">Peter Battaglia</a>,
              <a href="https://www.cs.ubc.ca/~nando/">Nando de Freitas</a>
              <br>
              <em>ICLR</em>, 2017
              <br>
              <br>
              <a href="https://arxiv.org/abs/1611.01843">arxiv</a> /
              <a href="https://www.newscientist.com/article/2112455-google-deepminds-ai-learns-to-play-with-physical-objects/">media</a> /
              <a href="https://www.youtube.com/watch?time_continue=1&amp;v=gs7mWG2sjUU&amp;feature=emb_logo" savefrom_lm_index="0" savefrom_lm="1"> video </a><span style="padding: 0; margin: 0; margin-left: 5px;"><a href="http://savefrom.net/?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Ftime_continue%3D1%26v%3Dgs7mWG2sjUU%26feature%3Demb_logo&amp;utm_source=userjs-chrome&amp;utm_medium=extensions&amp;utm_campaign=link_modifier" target="_blank" title="Get a direct link" savefrom_lm="1" savefrom_lm_is_link="1" style="background-image: url(&quot;data:image/gif;base64,R0lGODlhEAAQAOZ3APf39+Xl5fT09OPj4/Hx8evr6/3+/u7u7uDh4OPi497e3t7e3/z8/P79/X3GbuXl5ubl5eHg4WzFUfb39+Pj4lzGOV7LOPz7+/n6+vn5+ZTLj9/e387Ozt7f3/7+/vv7/ISbePn5+m/JV1nRKXmVbkCnKVrSLDqsCuDh4d/e3uDn3/z7/H6TdVeaV1uSW+bn5v39/eXm5eXm5kyHP/f39pzGmVy7J3yRd9/f3mLEKkXCHJbka2TVM5vaZn6Wdfn6+YG/c/r5+ZO/jeLi41aHTIeageLn4f39/vr6+kzNG2PVM5i+lomdf2CXYKHVmtzo2YXNeDqsBebl5uHh4HDKWN3g3kKqEH6WeZHTXIPKdnSPbv79/pfmbE7PHpe1l4O8dTO5DODg4VDLIlKUUtzo2J7SmEWsLlG4NJbFjkrJHP7+/VK5Nfz8+zmnC3KKa+Hg4OHh4Y63j/3+/eDg4Ojo6P///8DAwP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAEAAHcALAAAAAAQABAAAAfWgHd2g4SFhYJzdYqLjIpzgx5bBgYwHg1Hk2oNDXKDFwwfDF5NLmMtcStsn4MhGT8YS04aGmU1QRhIGYMTADQAQlAODlloAMYTgwICRmRfVBISIkBPKsqDBAREZmcVFhYVayUz2IMHB1dWOmImI2lgUVrmgwUFLzdtXTxKSSduMfSD6Aik48MGlx05SAykM0gKhAAPAhTB0oNFABkPHg5KMIBCxzlMQFQZMGBIggSDpsCJgGDOmzkIUCAIM2dOhEEcNijQuQDHgg4KOqRYwMGOIENIB90JBAA7&quot;); background-repeat: no-repeat; width: 16px; height: 16px; display: inline-block; border: none; text-decoration: none; padding: 0px; position: relative;"></a></span> /
              <a href="https://people.csail.mit.edu/pulkitag/data/denil2017learning.bib">bibtex</a>
              <p></p>
              <p> Deep reinforcement learning can equip an agent with the ability
                to perform experiments for inferring physical quanities of interest.
               </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <a href="https://www.ncbi.nlm.nih.gov/pubmed/29042342">
                  <img src="./Pulkit Agrawal_files/safely_cameras.png" alt="sym" width="100%" style="border-radius:5px">
              </a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ncbi.nlm.nih.gov/pubmed/29042342">
              <papertitle>Reduction in Fall Rate in Dementia Managed Care through
                 Video Incident Review: Pilot Study</papertitle>
              </a>
              <br>
              Eleonore Bayen, Julien Jacquemot, George Netscher,
              <strong>Pulkit Agrawal</strong>,
              Lynn Tabb Noyce, Alexandre Bayen
              <br>
              <em>Journal of Medical Internet Research</em>, 2017
              <br>
              <br>
              <a href="https://www.jmir.org/2017/10/e339/pdf">paper </a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/bayen2017reduction.bib">bibtex</a>
              <p></p>
              <p> Analysis how continuous video monitoring and review of falls
                of individuals with dementia can support better quality of care.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/ief.gif" alt="sym" width="80%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1507.06550">
                <papertitle>Human Pose Estimation with Iterative Error Feedback</papertitle>
              </a>
              <br>
              <a href="https://uk.linkedin.com/in/jo%C3%A3o-carreira-56238a7">Joao Carreira</a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>CVPR</em>, 2016  &nbsp; <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <br>
              <a href="https://arxiv.org/pdf/1507.06550">arxiv</a> /
              <a href="https://github.com/pulkitag/ief">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/carreira2016human.bib">bibtex</a>
              <p></p>
              <p> Iterative Error Feedback (IEF) is a self-correcting model that
                progressively changes an initial solution by feeding back error predictions.
                In contrast to feedforward CNNs that only capture structure in inputs,
                IEF captures structure in both the space of inputs and outputs.</p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/poking.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://ashvin.me/pokebot-website/">
              <papertitle>Learning to Poke by Poking: Experiential Learning of Intuitive Physics</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal*</strong>,
              <a href="http://ashvin.me/">Ashvin Nair*</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>(*equal contribution)
              <br>
              <em>NIPS</em>, 2016, <font color="red"><strong>(Oral)</strong></font>
              <br>
              <br>
              <a href="https://arxiv.org/abs/1505.01596">arxiv</a> /
              <a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Learning-to-Poke-by-Poking-Experiential-Learning-of-Intuitive-Physics">talk</a> /
              <a href="http://ashvin.me/pokebot-website/">project website</a> /
              <a href="https://drive.google.com/file/d/0B3xZefNMOTwuTUwtU0ZnaDhGVUE/view?usp=sharing">data</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/agrawal2016learning.bib">bibtex</a>
              <p></p>
              <p> Robot learns how to push objects to target locations by conducting
                a large number of pushing experiments. The code for the paper
                <em> Zero Shot Visual Imitation </em> subsumes this project's code release. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/imagenet_thumb.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1608.08614v2.pdf">
              <papertitle>What makes Imagenet Good for Transfer Learning?</papertitle>
              </a>
              <br>
              <a href="http://minyounghuh.com/"> Jacob Huh </a>,
              <strong>Pulkit Agrawal</strong>,
              <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>
              <br>
              <em>NIPS LSCVS Workshop</em>, 2016, &nbsp; <font color="red"><strong>(Oral)</strong></font>
              <br>
              <br>
              <a href="https://arxiv.org/pdf/1608.08614">arxiv</a> /
              <a href="http://minyounghuh.com/papers/analysis/">project website</a> /
              <a href="https://github.com/minyoungg/WMIGFT">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/huh2016what.bib">bibtex</a>
              <p></p>
              <p> An empirical investigation into various factors related to the
              statistics of Imagenet dataset that result in transferrable features. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/billiards.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1511.07404">
              <papertitle>Learning Visual Predictive Models of Physics for Playing Billiards</papertitle>
              </a>
              <br>
              <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki*</a>,
              <strong>Pulkit Agrawal*</strong>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>(*equal contribution)
              <br>
              <em>ICLR</em>, 2016
              <br>
              <br>
              <a href="https://arxiv.org/abs/1511.07404">arxiv</a> /
              <a href="https://github.com/pulkitag/pyphy-engine">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/fragkiadaki2016learning.bib">bibtex</a>
              <p></p>
              <p> This work explores how an agent can be equipped with an internal
                model of the dynamics of the external world, and how it can use this model to plan novel
                actions by running multiple internal simulations (visual imagination). </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./Pulkit Agrawal_files/3D_rep.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://3drepresentation.stanford.edu/">
              <papertitle>Generic 3d Representation via Pose Estimation and Matching</papertitle>
              </a>
              <br>
              <a href="https://cs.stanford.edu/~amirz/">Amir R. Zamir</a>,
              <a href="https://www.researchgate.net/profile/Tilman_Wekel"> Tilman Wekel</a>,
              <strong>Pulkit Agrawal</strong>,
              Colin Weil,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="http://cvgl.stanford.edu/silvio/"> Silvio Savarese</a>
              <br>
              <em>ECCV</em>, 2016
              <br>
              <br>
              <a href="https://arxiv.org/abs/1710.08247">arxiv</a> /
              <a href="http://3drepresentation.stanford.edu/">website </a> /
              <a href="https://github.com/amir32002/3D_Street_View"> dataset </a> /
              <a href="https://github.com/pulkitag/learning-to-see-by-moving">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/zamir2016generic.bib">bibtex</a>
              <p></p>
              <p> Large-scale study of feature learning using agent's knowledge of its motion.
                This paper extends our ICCV 2015 paper.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/lsm_3.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Agrawal_Learning_to_See_ICCV_2015_paper.pdf">
              <papertitle>Learning to See by Moving</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal</strong>,
              <a href="https://uk.linkedin.com/in/jo%C3%A3o-carreira-56238a7">Joao Carreira</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>ICCV</em>, 2015
              <br>
              <br>
              <a href="https://arxiv.org/abs/1505.01596">arxiv</a> /
              <a href="https://github.com/pulkitag/learning-to-see-by-moving">code</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/agrawal2015learning.bib">bibtex</a>
              <p></p>
              <p> Feature learning by making use of an agent's knowledge of its motion.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/analyzing_2014.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/papers/PulkitECCV2014.pdf">
                <papertitle>Analyzing the Performance of Multilayer Neural Networks for Object Recognition</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal</strong>,
              <a href="https://www.rossgirshick.info/">Ross Girshick</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
              <br>
              <em>ECCV</em>, 2014
              <br>
              <br>
              <a href="https://arxiv.org/abs/1407.1610">arxiv</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/carreira2016human.bib">bibtex</a>
              <p></p>
              <p> A detailed study of how to finetune neural networks and the
              nature of the learned representations.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/brain.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1407.5104">
              <papertitle>Pixels to Voxels: Modeling Visual Representation in the Human Brain</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal</strong>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Dustin Stansbury</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>,
              <a href="https://people.eecs.berkeley.edu/~malik/">Jack Gallant</a>
              <br>(*equal contribution)
              <br>
              <em>arXiv</em>, 2014
              <br>
              <br>
              <a href="https://arxiv.org/pdf/1407.5104">arxiv</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/cnn_mimics_brain.pdf">unpublished results</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/agrawal2014pixels.bib">bibtex</a>
              <p></p>
              <p> Comparing the representations learnt by a Deep Neural Network
              optimized for object recognition against the human brain. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./Pulkit Agrawal_files/automatic_assesment.png" alt="sym" width="100%" style="border-radius:5px">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pdfs.semanticscholar.org/0242/a90f7a269ef2d185ca59ada28e4160b638a6.pdf">
              <papertitle>The Automatic Assessment of Knowledge Integration Processes in Project Teams</papertitle>
              </a>
              <br>
              Gahgene Gweon,
              <strong>Pulkit Agrawal</strong>,
              Mikesh Udani,
              Bhiksha Raj,
              Carolyn Rose
              <br>
              <em>Computer Supported Collaborative Learning </em>, 2011
              &nbsp; <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <br>
              <a href="https://pdfs.semanticscholar.org/0242/a90f7a269ef2d185ca59ada28e4160b638a6.pdf">arxiv</a> /
              <a href="https://people.csail.mit.edu/pulkitag/data/gweon2011automatic.bib">bibtex</a>
              <p></p>
              <p> Method for identifying important parts of a group conversation
                directly from speech data. </p>
            </td>
          </tr>

          </tbody></table><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;vertical-align:middle">
              <heading>Patents</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://patentimages.storage.googleapis.com/5d/98/08/56f1af274703b6/US20190287376A1.pdf">
              <papertitle>System and Method for Detecting, Recording and Communicating
                Events in the Care and Treatment of Cognitively Impaired Persons</papertitle>
              </a>
              <br>
              George Netscher, Julien Jacquemot, <strong>Pulkit Agrawal</strong>, Alexandre Bayen
              <br>
              US Patent: <em>US20190287376A1</em>, 2019
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://patentimages.storage.googleapis.com/51/c6/fa/ce05ea8879dd68/US20150278628A1.pdf">
              <papertitle>Invariant Object Representation of Images Using Spiking Neural Networks</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal</strong>, Somdeb Majumdar, Vikram Gupta
              <br>
              US Patent: <em>US20150278628A1</em>, 2015
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://patentimages.storage.googleapis.com/cb/1f/cf/5d728862f69828/US20150278641A1.pdf">
              <papertitle>Invariant Object Representation of Images Using Spiking Neural Networks</papertitle>
              </a>
              <br>
              <strong>Pulkit Agrawal</strong>, Somdeb Majumdar
              <br>
              US Patent: <em>US20150278641A1</em>, 2015
            </td>
          </tr>

        </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            Area Chair, CoRL 2020 <br>
            Reviewer for CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR,
            ICRA, IJRR, IJCV, IEEE RA-L, TPAMI etc.
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">template</a>
            </p></td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>



</body></html>